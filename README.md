# Building-NanoGPT
Following GPT-2, GPT-3 and Attention is All You Need papers aswell as a tutorial from Andrej Karpathy to create a 124M parameter version of GPT-2
